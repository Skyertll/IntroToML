{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf0d66b6",
   "metadata": {},
   "source": [
    "(HW3)=\n",
    "# HW3\n",
    "\n",
    "## Task 3.1 (0.5 points)\n",
    "\n",
    "Express $\\sigma''(x)$ in terms of $\\sigma(x)$ where $\\sigma(x) = \\frac 1{1 + e^{-t}}$ â€” sigmoid function. Find all $x$ for which $\\sigma''(x) = 0$. Also find intervals on which sigmoid is convex/concave.\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "\n",
    "To express $\\sigma''(x) in terms of $\\sigma(x), where $\\sigma(x) = \\frac{1}{1 + e^{-x}}$, and determine the points where $\\sigma''(x) = 0$ and the intervals of convexity/concavity, follow these steps:\n",
    "\n",
    "First derivative, $\\sigma'(x)$: $\\frac{e^{-x}}{(1 + e^{-x})^2}$\n",
    "Second derivative, $\\sigma''(x)$: $-\\frac{e^{-x}}{(1 + e^{-x})^2} + \\frac{2e^{-2x}}{(1 + e^{-x})^3}$\n",
    "\n",
    "### Let`s find $\\sigma''(x) = 0$\n",
    "The sigmoid function's second derivative obviously does not equal zero at any specific value of $x$, indicating no real solutions for points of zero curvature change.\n",
    "\n",
    "### Convexity/Concavity Intervals\n",
    "- **Concave** for $x < 0$: $\\sigma''(x) < 0$\n",
    "- **Convex** for $x > 0$: $\\sigma''(x) > 0$\n",
    "\n",
    "The sigmoid function transitions from concave to convex at $x = 0$, which is the inflection point. This behavior showcases the sigmoid's property of having a smooth, S-shaped curve, which is widely utilized in various fields like machine learning for modeling logistic growth.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebd2b22",
   "metadata": {},
   "source": [
    "## Task 3.2 (0.5 points)\n",
    "\n",
    "Let $\\boldsymbol X \\in \\mathbb R^{m\\times n}$ and $f(\\boldsymbol X) = \\vert \\boldsymbol X \\vert^k$, $k\\in\\mathbb N$. Find $\\nabla f(\\boldsymbol X)$.\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "\n",
    "To find the gradient $\\nabla f(\\boldsymbol X)$ of $f(\\boldsymbol X) = |\\boldsymbol X|^k$, where $|\\boldsymbol X|$ is the Frobenius norm of the matrix $\\boldsymbol X \\in \\mathbb{R}^{m \\times n}$ and $k$ is a natural number, we utilize the Frobenius norm definition and differentiation rules.\n",
    "\n",
    "Given $f(\\boldsymbol X) = \\left(\\sum_{i=1}^{m}\\sum_{j=1}^{n} x_{ij}^2\\right)^{\\frac{k}{2}}$, the gradient of $f$ with respect to $\\boldsymbol X$ is:\n",
    "\n",
    "$\n",
    "\\nabla f(\\boldsymbol X) = k \\cdot \\boldsymbol X \\cdot \\left(\\sum_{i=1}^{m}\\sum_{j=1}^{n} x_{ij}^2\\right)^{\\frac{k}{2} - 1}\n",
    "$\n",
    "\n",
    "This indicates that each element of $\\boldsymbol X$ is scaled by $k$ and the Frobenius norm of $\\boldsymbol X$ raised to the power of $k - 2$, reflecting how changes in $\\boldsymbol X$ influence $f(\\boldsymbol X)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b0a12c",
   "metadata": {},
   "source": [
    "## Task 3.3 (2 points)\n",
    "\n",
    "Let $\\boldsymbol A \\in \\mathbb R^{n\\times n}$ be a symmetric matrix and\n",
    "\n",
    "$$\n",
    "f(\\boldsymbol x) = \\frac{\\boldsymbol x^\\mathsf{T} \\boldsymbol{Ax}}{\\boldsymbol x^\\mathsf{T} \\boldsymbol x}, \\quad \\boldsymbol x \\in \\mathbb R^n.\n",
    "$$ \n",
    "\n",
    "(a) (**1 point**) Calculate $\\nabla f(\\boldsymbol x)$ and prove that $\\nabla f(\\boldsymbol x) = \\boldsymbol 0$ iff $\\boldsymbol x$ is an eigenvector of $\\boldsymbol A$.\n",
    "\n",
    "(b) (**1 point**) Prove that if $\\boldsymbol A$ is positive definite then\n",
    "\n",
    "$$\n",
    "\\max\\limits_{\\boldsymbol x \\ne \\boldsymbol 0} f(\\boldsymbol x) = \\lambda_{\\max}(\\boldsymbol A), \\quad\n",
    "\\min\\limits_{\\boldsymbol x \\ne \\boldsymbol 0} f(\\boldsymbol x) = \\lambda_{\\min}(\\boldsymbol A)\n",
    "$$\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "\n",
    "### Part (a)\n",
    "\n",
    "For $f(\\boldsymbol x) = \\frac{\\boldsymbol x^\\mathsf{T} \\boldsymbol{Ax}}{\\boldsymbol x^\\mathsf{T} \\boldsymbol x}$,\n",
    "\n",
    "$\n",
    "\\nabla f(\\boldsymbol x) = \\frac{2\\boldsymbol{Ax}(\\boldsymbol x^\\mathsf{T} \\boldsymbol x) - 2\\boldsymbol x (\\boldsymbol x^\\mathsf{T} \\boldsymbol{Ax})}{(\\boldsymbol x^\\mathsf{T} \\boldsymbol x)^2}\n",
    "$\n",
    "\n",
    "$\\nabla f(\\boldsymbol x) = \\boldsymbol 0$ iff $\\boldsymbol{Ax} = \\lambda \\boldsymbol x$.\n",
    "\n",
    "The gradient of $f(\\boldsymbol x)$, $\\nabla f(\\boldsymbol x)$, becomes zero if and only if $\\boldsymbol x$ is an eigenvector of $\\boldsymbol A$, indicating that the function's rate of change in all directions is zero at this point. This condition implies that $\\boldsymbol{Ax} = \\lambda \\boldsymbol x$, where $\\lambda$ is the eigenvalue corresponding to eigenvector $\\boldsymbol x$.\n",
    "\n",
    "### Part (b)\n",
    "\n",
    "For $\\boldsymbol A$ positive definite,\n",
    "\n",
    "$\n",
    "\\max\\limits_{\\boldsymbol x \\ne \\boldsymbol 0} f(\\boldsymbol x) = \\lambda_{\\max}(\\boldsymbol A)\n",
    "$\n",
    "\n",
    "$\n",
    "\\min\\limits_{\\boldsymbol x \\ne \\boldsymbol 0} f(\\boldsymbol x) = \\lambda_{\\min}(\\boldsymbol A)\n",
    "$\n",
    "\n",
    "Where $\\lambda_{\\max}$, $\\lambda_{\\min}$ are the maximum and minimum eigenvalues of $\\boldsymbol A$, respectively.\n",
    "\n",
    "The Rayleigh quotient $f(\\boldsymbol x)$ reaches its maximum and minimum values when $\\boldsymbol x$ aligns with the eigenvectors corresponding to $\\boldsymbol A$'s largest and smallest eigenvalues, respectively. This result stems from the properties of symmetric and positive definite matrices, where the spread of the function's values is determined by the eigenvalues of $\\boldsymbol A$. Thus, the maximum and minimum of $f(\\boldsymbol x)$ are $\\lambda_{\\max}(\\boldsymbol A)$ and $\\lambda_{\\min}(\\boldsymbol A)$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34afdb9",
   "metadata": {},
   "source": [
    "## Task 3.4 (1 point)\n",
    "\n",
    "4. Find \n",
    "\n",
    "    $$\n",
    "        \\min\\limits_{\\boldsymbol X} \\Vert \\boldsymbol{AX} - \\boldsymbol B  \\Vert_F^2, \\quad \\boldsymbol A \\in \\mathbb R^{k\\times m},\\quad \\boldsymbol B \\in \\mathbb R^{k\\times n},\\quad \\mathrm{rank}(\\boldsymbol A) = m.\n",
    "    $$\n",
    "\n",
    "### YOUR SOLUTION  HERE\n",
    "\n",
    "To minimize $f(\\boldsymbol X) = \\Vert \\boldsymbol{AX} - \\boldsymbol B \\Vert_F^2$, where $\\boldsymbol A \\in \\mathbb R^{k\\times m}$, $\\boldsymbol B \\in \\mathbb R^{k\\times n}$, and $\\mathrm{rank}(\\boldsymbol A) = m$, we use the condition for optimality:\n",
    "\n",
    "$\n",
    "\\nabla_{\\boldsymbol X} f(\\boldsymbol X) = 0 \\implies 2\\boldsymbol A^\\top(\\boldsymbol{AX} - \\boldsymbol B) = 0\n",
    "$\n",
    "\n",
    "$\\boldsymbol A^\\top\\boldsymbol{AX} = \\boldsymbol A^\\top\\boldsymbol B$\n",
    "\n",
    "$\n",
    "\\boldsymbol X = (\\boldsymbol A^\\top\\boldsymbol A)^{-1}\\boldsymbol A^\\top\\boldsymbol B\n",
    "$\n",
    "\n",
    "This formula provides the $\\boldsymbol X$ that minimizes the given function, representing the best approximation to the equation $\\boldsymbol{AX} \\approx \\boldsymbol B$ under the Frobenius norm."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
